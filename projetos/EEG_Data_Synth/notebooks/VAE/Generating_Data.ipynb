{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faa542c1-4602-46f7-9a65-50891479dead",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-11-17 18:23:16.440654: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-17 18:23:16.440744: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-17 18:23:16.440805: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-17 18:23:16.447651: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow not install, you could not use those pipelines\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms, utils\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import TensorDataset\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import scale as standard_scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skorch.helper import predefined_split\n",
    "from skorch.callbacks import LRScheduler\n",
    "from skorch.dataset import ValidSplit\n",
    "\n",
    "# torch.manual_seed(0) # Set for our testing purposes, please do not change!\n",
    "\n",
    "from braindecode.datasets import MOABBDataset\n",
    "from braindecode.preprocessing import (\n",
    "    exponential_moving_standardize, preprocess, Preprocessor)\n",
    "from braindecode.preprocessing import \\\n",
    "    create_windows_from_events, create_fixed_length_windows\n",
    "from braindecode.models import EEGNetv4\n",
    "from braindecode import EEGClassifier\n",
    "import mne\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cafd8ed-829f-4a8d-b760-7972c60aae2d",
   "metadata": {},
   "source": [
    "# Classe Gerador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5533c42a-be95-45cc-b159-f2ea4588bf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    Generator Class\n",
    "    Values:\n",
    "        input_dim: the dimension of the input vector, a scalar\n",
    "        im_chan: the number of channels of the output eeg, a scalar\n",
    "        hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "    def __init__(self, input_dim=68, im_chan=1, hidden_dim=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        # Build the neural network\n",
    "        self.gen = nn.Sequential(\n",
    "            #### For 3 channels\n",
    "#             self.make_gen_block(input_dim, hidden_dim * 4,      kernel_size = (1,60), stride = (1,1)),\n",
    "#             self.make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size = (1,60), stride = (1,1)),\n",
    "#             self.make_gen_block(hidden_dim * 2, hidden_dim,     kernel_size = (1,60), stride = (1,1)),\n",
    "#             self.make_gen_block(hidden_dim, im_chan,            kernel_size = (3,50), stride = (1,2), padding = (0,2), final_layer=True),\n",
    "            #### For 22 channels\n",
    "            self.make_gen_block(input_dim, hidden_dim * 4,      kernel_size = (3,60), stride = (1,1)),\n",
    "            self.make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size = (4,60), stride = (3,1)),\n",
    "            self.make_gen_block(hidden_dim * 2, hidden_dim,     kernel_size = (3,60), stride = (2,1)),\n",
    "            self.make_gen_block(hidden_dim, im_chan,            kernel_size = (2,50), stride = (1,2), padding = (0,2), final_layer=True),\n",
    "        )\n",
    "\n",
    "    def make_gen_block(self, input_channels, output_channels, kernel_size, stride, padding = 0, final_layer=False):\n",
    "        '''\n",
    "        Function to return a sequence of operations corresponding to a generator block of DCGAN;\n",
    "        a transposed convolution, a batchnorm (except in the final layer), and an activation.\n",
    "        Parameters:\n",
    "            input_channels: how many channels the input feature representation has\n",
    "            output_channels: how many channels the output feature representation should have\n",
    "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
    "            stride: the stride of the convolution\n",
    "            final_layer: a boolean, true if it is the final layer and false otherwise \n",
    "                      (affects activation and batchnorm)\n",
    "        '''\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride, padding),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride, padding),\n",
    "                nn.Tanh(),\n",
    "            )\n",
    "\n",
    "    def forward(self, noise):\n",
    "        '''\n",
    "        Function for completing a forward pass of the generator: Given a noise tensor, \n",
    "        returns generated images.\n",
    "        Parameters:\n",
    "            noise: a noise tensor with dimensions (n_samples, input_dim)\n",
    "        '''\n",
    "        x = noise.view(len(noise), self.input_dim, 1, 1)\n",
    "        return self.gen(x)\n",
    "\n",
    "def get_noise(n_samples, input_dim, device='cpu'):\n",
    "    '''\n",
    "    Function for creating noise vectors: Given the dimensions (n_samples, input_dim)\n",
    "    creates a tensor of that shape filled with random numbers from the normal distribution.\n",
    "    Parameters:\n",
    "        n_samples: the number of samples to generate, a scalar\n",
    "        input_dim: the dimension of the input vector, a scalar\n",
    "        device: the device type\n",
    "    '''\n",
    "    return torch.randn(n_samples, input_dim, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbb3f55-9af4-4ca8-889d-ce0a8ef5fcce",
   "metadata": {},
   "source": [
    "# Função para gerar dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb7d667b-9771-4ee0-a75e-5bfc1edd046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_labels(labels, n_classes):\n",
    "    '''\n",
    "    Function for creating one-hot vectors for the labels, returns a tensor of shape (?, num_classes).\n",
    "    Parameters:\n",
    "        labels: tensor of labels from the dataloader, size (?)\n",
    "        n_classes: the total number of classes in the dataset, an integer scalar\n",
    "    '''\n",
    "    return F.one_hot(labels,n_classes)\n",
    "\n",
    "def combine_vectors(x, y):\n",
    "    '''\n",
    "    Function for combining two vectors with shapes (n_samples, ?) and (n_samples, ?).\n",
    "    Parameters:\n",
    "      x: (n_samples, ?) the first vector. \n",
    "        In this assignment, this will be the noise vector of shape (n_samples, z_dim), \n",
    "        but you shouldn't need to know the second dimension's size.\n",
    "      y: (n_samples, ?) the second vector.\n",
    "        Once again, in this assignment this will be the one-hot class vector \n",
    "        with the shape (n_samples, n_classes), but you shouldn't assume this in your code.\n",
    "    '''\n",
    "    combined = torch.cat((x.float(),y.float()), 1)\n",
    "    return combined\n",
    "\n",
    "\n",
    "def generate_samples_with_labels(label, n_samples, generator, z_dim = 64, channel = None, extra_dim = True):\n",
    "    '''\n",
    "    Function for generating samples, once the generator has been trained\n",
    "        label: label of the movement to be sampled. See dictionary below\n",
    "        {'feet': 0, 'left_hand': 1, 'right_hand': 2, 'tongue': 3}\n",
    "        n_samples: number of samples to be generated\n",
    "        channel: electrode {'C3': 0, 'Cz': 1, 'C4': 2} -> Default: All channels\n",
    "        generator: the trained generator\n",
    "    '''\n",
    "\n",
    "    n_classes = 4\n",
    "\n",
    "    if channel == None:\n",
    "        noise_4_gen = get_noise(n_samples, z_dim)\n",
    "        label = get_one_hot_labels(torch.Tensor([label]).long(), n_classes).repeat(n_samples,1)\n",
    "\n",
    "        noise_and_labels = combine_vectors(noise_4_gen, label)\n",
    "        fake = generator(noise_and_labels)\n",
    "\n",
    "        if extra_dim == False:\n",
    "            fake = fake.reshape((fake.shape[0], fake.shape[2], fake.shape[3]))\n",
    "        return fake\n",
    "    else:\n",
    "        noise_4_gen = get_noise(n_samples, z_dim)\n",
    "        label = get_one_hot_labels(torch.Tensor([label]).long(), n_classes).repeat(n_samples,1)\n",
    "\n",
    "        noise_and_labels = combine_vectors(noise_4_gen, label)\n",
    "        fake = generator(noise_and_labels)\n",
    "        filtered_channel_fake = torch.select(fake, dim = 2, index = channel)\n",
    "\n",
    "        if extra_dim == False:\n",
    "            filtered_channel_fake = filtered_channel_fake.reshape((filtered_channel_fake.shape[0], filtered_channel_fake.shape[2]))\n",
    "\n",
    "        return filtered_channel_fake\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d81459a-3bc1-4b83-8d76-f86c2c53a0c3",
   "metadata": {},
   "source": [
    "# Configuração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8fdd608-54b3-4a40-8a4a-d4fd992914f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = \"./gen_22ch_batch64_lr37e6_decay7.pt\"\n",
    "gen = Generator()\n",
    "gen.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19361e79-6c19-4a16-a300-0b6a23cdb1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tamanho_sample = 10 # Deixar 288\n",
    "\n",
    "gen_data_label0 = generate_samples_with_labels(generator = gen, label = 0,n_samples = tamanho_sample)\n",
    "gen_data_label1 = generate_samples_with_labels(generator = gen, label = 1,n_samples = tamanho_sample)\n",
    "gen_data_label2 = generate_samples_with_labels(generator = gen, label = 2,n_samples = tamanho_sample)\n",
    "gen_data_label3 = generate_samples_with_labels(generator = gen, label = 3,n_samples = tamanho_sample)\n",
    "\n",
    "# Falta concatenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "98366957-f527-4efe-b7e8-b1cff98eb646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 22, 400])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_data_label3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f333b406-8fee-4fbd-add8-e7771181e4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat\n",
    "gen_data_final = torch.cat((gen_data_label0,\n",
    "                          gen_data_label1,\n",
    "                          gen_data_label2,\n",
    "                          gen_data_label3), dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "68168093-d729-4eba-8da4-446cf69c31de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 1, 22, 400])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_data_final.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
